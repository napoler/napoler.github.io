---
layout: post
comments: 1
title:  2021-06-15 记事docker边测试边开发
categories: Default
tags: docker
date: 2021-06-15 15:42
---

 2021-06-15 记事

docker测试效果

```
docker run --rm -it --entrypoint bash <image-name-or-id>

#示例
docker run --rm  -p 8097:80 -it benchmark  bash
```

```
docker exec -it <container-name-or-id> bash
```


## 微调 #Bart 做 #知识提取
学习率貌似真的不能够很大，不然很容易走向不归路。



![](img/W&B%20Chart%202021_6_15%20下午9_39_32%201.svg)

自动寻找学习率靠谱吗？
话说每次出来的 #学习率 差距都很大怎么选？

9e-6是不是太小了，不过好像表现最好。


腾讯 #uer 家的 #预训练 还是不错的。
[https://huggingface.co/uer](https://huggingface.co/uer)


#huggingface bart 训练参考

[https://huggingface.co/transformers/model\_doc/bart.html](https://huggingface.co/transformers/model_doc/bart.html)

训练cnn_dailymail示例；

[https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization](https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization)


[https://github.com/ohmeow/ohmeow\_website/blob/master/\_notebooks/2020-05-23-text-generation-with-blurr.ipynb](https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb)

代码
[https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run\_summarization.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run_summarization.py)
```python
# 这里需要关注

from transformers import (

AutoConfig,

AutoModelForSeq2SeqLM,

AutoTokenizer,

DataCollatorForSeq2Seq,

HfArgumentParser,

Seq2SeqTrainer,

Seq2SeqTrainingArguments,

set\_seed,

)

```

还是使用 #pytorch-lightning 重写下比较好，条理会更加清晰。

## #Git LFS

> 
每个使用 Git Large File Storage 的帐户都会获得 1 GB 的免费存储空间和一个月的免费带宽 1 GB。 如果带宽和存储配额不够，您可以选择为 Git LFS 购买额外配额。

[https://docs.github.com/cn/github/managing-large-files/versioning-large-files/about-storage-and-bandwidth-usage](https://docs.github.com/cn/github/managing-large-files/versioning-large-files/about-storage-and-bandwidth-usage)

对于大文件是不是有点鸡肋了？


## #Transformers #Notebooks
这里绝对的宝藏 #nlp 
[https://huggingface.co/transformers/v3.0.2/notebooks.html](https://huggingface.co/transformers/v3.0.2/notebooks.html)

#Pipelines 绝对需要关注下
[How to use Pipelines](https://github.com/huggingface/transformers/blob/master/notebooks/03-pipelines.ipynb)



## #pytorch-lightning-bolts
这个需要关注下
![](https://warehouse-camo.ingress.cmh1.psfhosted.org/6a83ecf4a3fba7fa5074029b5228a4e1a9fe4aa1/68747470733a2f2f6769746875622e636f6d2f5079546f7263684c696768746e696e672f6c696768746e696e672d626f6c74732f7261772f302e332e322f646f63732f736f757263652f5f696d616765732f6c6f676f732f626f6c74735f6c6f676f2e706e67)
> pip install pytorch-lightning-bolts

**Pretrained SOTA Deep Learning models, callbacks and more for research and production with PyTorch Lightning and PyTorch**

集成了好多模型，直接拿来就ok，另外自己定制也可以参考或者作为基础。

https://github.com/PyTorchLightning/lightning-bolts

[https://pytorch-lightning-bolts.readthedocs.io/](https://pytorch-lightning-bolts.readthedocs.io/)


