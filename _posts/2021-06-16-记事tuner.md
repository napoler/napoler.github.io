---
layout: post
comments: 1
title:  pytorch_lightning tuner
categories: Default
tags: Default
date: 2021-06-16 08:48
---

 2021-06-16 记事
 
 ## pytorch_lightning model_checkpoint


```python

# # 自动批次大小 https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/tuner/tuning.html
# tuner = pl.tuner.tuning.Tuner(trainer)
# # Invoke method
# new_batch_size = tuner.scale_batch_size(model)
# # Override old batch size (this is done automatically)
# model.hparams.batch_size = new_batch_size

# print(model.hparams)




trainer = pl.Trainer(
    gpus=1,
#     min_epochs=1,
    precision=16,amp_level='O2',
    checkpoint_callback=checkpoint_callback,
    resume_from_checkpoint="/kaggle/input/LitAutoMark-out.ckpt",
    auto_select_gpus=True,callbacks=[lr_monitor,early_stop_callback],
    deterministic=True,
    auto_scale_batch_size='binsearch',
    auto_lr_find=True,
    max_epochs=300,
    logger=wandb_logger,
    accumulate_grad_batches=accumulate_grad_batches)














```


## 深度学习代码中的 #随机种子
深度学习网络模型中初始的权值参数通常都是初始化成随机数
而使用梯度下降法最终得到的局部最优解对于初始位置点的选择很敏感
为了能够完全复现作者的开源深度学习代码，随机种子的选择能够减少一定程度上
算法结果的随机性，也就是更接近于原始作者的结果
即产生随机种子意味着每次运行实验，产生的随机数都是相同的
但是在大多数情况下，即使设定了随机种子，仍然没有办法完全复现
作者paper中所给出的模型性能，这是因为深度学习代码中除了产生随机数中带有随机性
其训练的过程中使用 mini-batch SGD或者优化算法进行训练时，本身就带有了随机性
因为每次更新都是从训练数据集中随机采样出batch size个训练样本计算的平均梯度
作为当前step对于网络权值的更新值，所以即使提供了原始代码和随机种子，想要
复现作者paper中的性能也是非常困难的
