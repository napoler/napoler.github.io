---
layout: post
title: 'pytorch版本的reformer库推荐 中文实现也很轻松reformer-pytorch-chinese'
permalink: '/2020/03/pytorchreformer.html'
comments: 1
categories: Default
tags: 自然语言 nlg pytorch reformer nlp
comments: 1
---
<span style='background-color: white; color: #24292e; font-family: , "blinkmacsystemfont" , "segoe ui" , "helvetica" , "arial" , sans-serif , "apple color emoji" , "segoe ui emoji"; font-size: 16px;'>Transformer很强大但是消耗资源有点多，还好google又搞出来了</span><span style='color: #24292e; font-family: , "blinkmacsystemfont" , "segoe ui" , "helvetica" , "arial" , sans-serif , "apple color emoji" , "segoe ui emoji";'>reformer在资源消耗上做了很大的优化，这也让我们能够以更小的代价进行尝试，毕竟GPU真的不便宜。</span>  
<span style='color: #24292e; font-family: , "blinkmacsystemfont" , "segoe ui" , "helvetica" , "arial" , sans-serif , "apple color emoji" , "segoe ui emoji";'>  
</span>[reformer-pytorch](https://github.com/lucidrains/reformer-pytorch)可以试用下  
  
<https://github.com/lucidrains/reformer-pytorch>  
  
配合transformers的BertTokenizer把文字转化成ids后直接交个<span style="color: #24292e;">reformer处理，很轻松的就可以实现一个gpt2效果一样的模型了。</span>  
<span style="color: #24292e;">  
</span><span style="color: #24292e;"><a href="https://colab.research.google.com/drive/1_YJpr-8pswWccFo01tbx1xZd3tKsf8td" rel="nofollow" target="_blank">colab上运行示例</a><span style="color: black;">&nbsp;</span></span>  

<blockquote class="tr_bq"><span style="color: #24292e;"><span style="color: black;"></span></span><br/><blockquote class="tr_bq"><span style="color: #24292e;"><span style="color: black;"><span style="color: #24292e;">import torch</span></span></span></blockquote><span style="color: #24292e;"><span style="color: black;"><span style="color: #24292e;"><blockquote class="tr_bq">from torch import randint</blockquote><blockquote class="tr_bq">from reformer_pytorch import ReformerLM</blockquote><blockquote class="tr_bq">from reformer_pytorch.generative_tools import TrainingWrapper</blockquote><blockquote class="tr_bq">from transformers import BertTokenizer</blockquote><blockquote class="tr_bq"><br/></blockquote><blockquote class="tr_bq"></blockquote><blockquote class="tr_bq">tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')&nbsp;</blockquote><blockquote class="tr_bq">tokenizer.max_len = 128</blockquote><blockquote class="tr_bq">model = ReformerLM(</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; num_tokens= tokenizer.vocab_size,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; dim = 128,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; depth = 12,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; max_seq_len = tokenizer.max_len ,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; lsh_dropout = 0.1,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; causal = True,</blockquote><blockquote class="tr_bq">&nbsp; &nbsp; full_attn_thres = 128</blockquote><blockquote class="tr_bq">)</blockquote><blockquote class="tr_bq"><br/></blockquote><blockquote class="tr_bq"># 0 is used for padding and no loss to be calculated on it</blockquote><blockquote class="tr_bq">model = TrainingWrapper(model, ignore_index = 0, pad_value = 0)</blockquote><blockquote class="tr_bq">#训练语料</blockquote><blockquote class="tr_bq">text_list=['你好吗','哈哈','我还好','我还好','我还好','我还好','我还好']</blockquote><blockquote class="tr_bq">x_train=[]</blockquote><blockquote class="tr_bq">for item in text_list:</blockquote><blockquote class="tr_bq">&nbsp; # x_train.append(torch.tensor(tokenizer.encode("你好吗", add_special_tokens=True)).unsqueeze(0))&nbsp; # Batch size 1</blockquote><blockquote class="tr_bq">&nbsp; tok = tokenizer.encode(item, max_length=tokenizer.max_len, add_special_tokens=True)</blockquote><blockquote class="tr_bq">&nbsp; tok = torch.tensor(tok, dtype=torch.long)</blockquote><blockquote class="tr_bq">&nbsp; # print(tok)</blockquote><blockquote class="tr_bq">&nbsp; x_train.append(tok)</blockquote><blockquote class="tr_bq">print(x_train)</blockquote><blockquote class="tr_bq"># when training, set return_loss equal to True</blockquote><blockquote class="tr_bq">model.train()</blockquote><blockquote class="tr_bq">loss = model(x_train, return_loss = True)</blockquote><blockquote class="tr_bq">loss.backward()</blockquote><blockquote class="tr_bq"><br/></blockquote><blockquote class="tr_bq"># when evaluating, just use the generate function, which will default to top_k sampling with temperature of 1.</blockquote><blockquote class="tr_bq">initial = torch.tensor([[0]]).long() # assume 0 is start token</blockquote><blockquote class="tr_bq">sample = model.generate(initial, 100, temperature=1., filter_thres = 0.9, eos_token = 1) # assume end token is 1, or omit and it will sample up to 100</blockquote><blockquote class="tr_bq">print(sample.shape) # (1, &lt;=100) token ids</blockquote><blockquote class="tr_bq">print(sample)</blockquote><blockquote class="tr_bq">text = tokenizer.convert_ids_to_tokens(sample.tolist()[0])</blockquote><blockquote class="tr_bq">print(text)</blockquote></span></span></span></blockquote>

<a href="https://colab.research.google.com/drive/1_YJpr-8pswWccFo01tbx1xZd3tKsf8td" rel="nofollow" target="_blank">colab上运行示例</a>