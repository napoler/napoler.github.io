
---
layout: post
comments: 1
title:  2021-06-13 记事
categories: Default
tags: Default
date: 2021-06-13 14:59
---

 2021-06-13 记事


## 　# ROUGE包评估汇总质量的Python包装器
[https://pypi.org/project/rouge/](https://pypi.org/project/rouge/)
 
 
 
 
 
 [https://colab.research.google.com/drive/1-OEwiD9ouGjWrSFEWhgEnWiNvwwxlqd7#scrollTo=no6DwOqaE9Jw](https://colab.research.google.com/drive/1-OEwiD9ouGjWrSFEWhgEnWiNvwwxlqd7#scrollTo=no6DwOqaE9Jw)
 
 ```
 def shift\_tokens\_right(input\_ids, pad\_token\_id):

""" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).

This is taken directly from modeling\_bart.py

"""

prev\_output\_tokens = input\_ids.clone()

index\_of\_eos = (input\_ids.ne(pad\_token\_id).sum(dim=1) - 1).unsqueeze(\-1)

prev\_output\_tokens\[:, 0\] = input\_ids.gather(1, index\_of\_eos).squeeze()

prev\_output\_tokens\[:, 1:\] = input\_ids\[:, :\-1\]

return prev\_output\_tokens

  

def encode\_sentences(tokenizer, source\_sentences, target\_sentences, max\_length\=32, pad\_to\_max\_length\=True, return\_tensors\="pt"):

''' Function that tokenizes a sentence

Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences

Returns: Dictionary with keys: input\_ids, attention\_mask, target\_ids

'''

  

input\_ids = \[\]

attention\_masks = \[\]

target\_ids = \[\]

tokenized\_sentences = {}

  

for sentence in source\_sentences:

encoded\_dict = tokenizer(

sentence,

max\_length=max\_length,

padding="max\_length" if pad\_to\_max\_length else None,

truncation=True,

return\_tensors=return\_tensors,

add\_prefix\_space = True

)

  

input\_ids.append(encoded\_dict\['input\_ids'\])

attention\_masks.append(encoded\_dict\['attention\_mask'\])

  

input\_ids = torch.cat(input\_ids, dim = 0)

attention\_masks = torch.cat(attention\_masks, dim = 0)

  

for sentence in target\_sentences:

encoded\_dict = tokenizer(

sentence,

max\_length=max\_length,

padding="max\_length" if pad\_to\_max\_length else None,

truncation=True,

return\_tensors=return\_tensors,

add\_prefix\_space = True

)

\# Shift the target ids to the right

\# shifted\_target\_ids = shift\_tokens\_right(encoded\_dict\['input\_ids'\], tokenizer.pad\_token\_id)

target\_ids.append(encoded\_dict\['input\_ids'\])

  

target\_ids = torch.cat(target\_ids, dim = 0)

  

batch = {

"input\_ids": input\_ids,

"attention\_mask": attention\_masks,

"labels": target\_ids,

}

  

return batch

  
  

def noise\_sentence(sentence\_, percent\_words, replacement\_token = "<mask>"):

'''

Function that noises a sentence by adding <mask> tokens

Args: sentence - the sentence to noise

percent\_words - the percent of words to replace with <mask> tokens; the number is rounded up using math.ceil

Returns a noised sentence

'''

\# Create a list item and copy

sentence\_ = sentence\_.split(' ')

sentence = sentence\_.copy()

num\_words = math.ceil(len(sentence) \* percent\_words)

\# Create an array of tokens to sample from; don't include the last word as an option because in the case of lyrics

\# that word is often a rhyming word and plays an important role in song construction

sample\_tokens = set(np.arange(0, np.maximum(1, len(sentence)\-1)))

words\_to\_noise = random.sample(sample\_tokens, num\_words)

\# Swap out words, but not full stops

for pos in words\_to\_noise:

if sentence\[pos\] != '.':

sentence\[pos\] = replacement\_token

\# Remove redundant spaces

sentence = re.sub(r' {2,5}', ' ', ' '.join(sentence))

\# Combine concurrent <mask> tokens into a single token; this just does two rounds of this; more could be done

sentence = re.sub(r'<mask> <mask>', "<mask>", sentence)

sentence = re.sub(r'<mask> <mask>', "<mask>", sentence)

return sentence
 
 ```
 
 ## kaggle上配合optuna
 
 中文文档地址
 
 https://optuna.readthedocs.io/zh_CN/latest/index.html
 
 [https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization](https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization)
 
 
 
 
 