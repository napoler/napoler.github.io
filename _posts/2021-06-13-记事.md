
---
layout: post
comments: 1
title:  2021-06-13 记事
categories: Default
tags: Default
date: 2021-06-13 14:59
---

 2021-06-13 记事


## 　# ROUGE包评估汇总质量的Python包装器
[https://pypi.org/project/rouge/](https://pypi.org/project/rouge/)
 
 
 
 
 
 [https://colab.research.google.com/drive/1-OEwiD9ouGjWrSFEWhgEnWiNvwwxlqd7#scrollTo=no6DwOqaE9Jw](https://colab.research.google.com/drive/1-OEwiD9ouGjWrSFEWhgEnWiNvwwxlqd7#scrollTo=no6DwOqaE9Jw)
 
 ```
 def shift\_tokens\_right(input\_ids, pad\_token\_id):

""" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).

This is taken directly from modeling\_bart.py

"""

prev\_output\_tokens = input\_ids.clone()

index\_of\_eos = (input\_ids.ne(pad\_token\_id).sum(dim=1) - 1).unsqueeze(\-1)

prev\_output\_tokens\[:, 0\] = input\_ids.gather(1, index\_of\_eos).squeeze()

prev\_output\_tokens\[:, 1:\] = input\_ids\[:, :\-1\]

return prev\_output\_tokens

  

def encode\_sentences(tokenizer, source\_sentences, target\_sentences, max\_length\=32, pad\_to\_max\_length\=True, return\_tensors\="pt"):

''' Function that tokenizes a sentence

Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences

Returns: Dictionary with keys: input\_ids, attention\_mask, target\_ids

'''

  

input\_ids = \[\]

attention\_masks = \[\]

target\_ids = \[\]

tokenized\_sentences = {}

  

for sentence in source\_sentences:

encoded\_dict = tokenizer(

sentence,

max\_length=max\_length,

padding="max\_length" if pad\_to\_max\_length else None,

truncation=True,

return\_tensors=return\_tensors,

add\_prefix\_space = True

)

  

input\_ids.append(encoded\_dict\['input\_ids'\])

attention\_masks.append(encoded\_dict\['attention\_mask'\])

  

input\_ids = torch.cat(input\_ids, dim = 0)

attention\_masks = torch.cat(attention\_masks, dim = 0)

  

for sentence in target\_sentences:

encoded\_dict = tokenizer(

sentence,

max\_length=max\_length,

padding="max\_length" if pad\_to\_max\_length else None,

truncation=True,

return\_tensors=return\_tensors,

add\_prefix\_space = True

)

\# Shift the target ids to the right

\# shifted\_target\_ids = shift\_tokens\_right(encoded\_dict\['input\_ids'\], tokenizer.pad\_token\_id)

target\_ids.append(encoded\_dict\['input\_ids'\])

  

target\_ids = torch.cat(target\_ids, dim = 0)

  

batch = {

"input\_ids": input\_ids,

"attention\_mask": attention\_masks,

"labels": target\_ids,

}

  

return batch

  
  

def noise\_sentence(sentence\_, percent\_words, replacement\_token = "<mask>"):

'''

Function that noises a sentence by adding <mask> tokens

Args: sentence - the sentence to noise

percent\_words - the percent of words to replace with <mask> tokens; the number is rounded up using math.ceil

Returns a noised sentence

'''

\# Create a list item and copy

sentence\_ = sentence\_.split(' ')

sentence = sentence\_.copy()

num\_words = math.ceil(len(sentence) \* percent\_words)

\# Create an array of tokens to sample from; don't include the last word as an option because in the case of lyrics

\# that word is often a rhyming word and plays an important role in song construction

sample\_tokens = set(np.arange(0, np.maximum(1, len(sentence)\-1)))

words\_to\_noise = random.sample(sample\_tokens, num\_words)

\# Swap out words, but not full stops

for pos in words\_to\_noise:

if sentence\[pos\] != '.':

sentence\[pos\] = replacement\_token

\# Remove redundant spaces

sentence = re.sub(r' {2,5}', ' ', ' '.join(sentence))

\# Combine concurrent <mask> tokens into a single token; this just does two rounds of this; more could be done

sentence = re.sub(r'<mask> <mask>', "<mask>", sentence)

sentence = re.sub(r'<mask> <mask>', "<mask>", sentence)

return sentence
 
 ```
 
 ## kaggle上配合optuna
 
 中文文档地址
 
 https://optuna.readthedocs.io/zh_CN/latest/index.html
 
 [https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization](https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization)
 
 
 
 ## 使用 #Ray #Tune 扩展 #Optuna
 [Optuna](https://optuna.org/)和[Ray Tune](http://tune.io/)是 Python[中超参数](https://optuna.org/)[调优](http://tune.io/)的两个主要工具。[Optuna](https://optuna.org/)为高级超参数搜索算法（如[Tree-Parzen Estimators ）](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html#optuna.samplers.TPESampler)提供了一个易于使用的界面。这使其成为现代机器学习工程师或数据科学家的宝贵工具，也是其受欢迎的关键原因。

[Optuna](https://optuna.org/)擅长单机工作负载，但并行处理这些工作负载需要手动操作，可能在多台机器上，并且不包括监控能力。如果您想[尽可能高效](https://docs.ray.io/en/master/tune/user-guide.html#parallelism-gpus)地[利用 GPU，](https://docs.ray.io/en/master/tune/user-guide.html#parallelism-gpus)这会使操作变得特别具有挑战性。您不仅需要明智地选择参数，还需要一种组织执行的方法。这就是[Ray Tune](http://tune.io/)闪耀的地方。

[Ray Tune](http://tune.io/)接受您的训练功能并自动将其并行化，负责[资源管理](https://docs.ray.io/en/master/tune/tutorials/overview.html#how-do-i-set-resources)，甚至可以将其分布[在一组机器上](https://docs.ray.io/en/master/cluster/index.html)。您所要做的就是运行一个脚本！在一个完美的世界中，我们将能够同时使用[Optuna](https://optuna.org/)的出色算法和[Ray Tune](http://tune.io/)的出色缩放功能。
 
 [https://medium.com/optuna/scaling-up-optuna-with-ray-tune-88f6ca87b8c7#:~:text=Ray%20Tune%20integrates%20with%20many,is%20one%20of%20these%20frameworks!&text=Even%20better%2C%20if%20you%20use,currently%20not%20available%20in%20Optuna.](https://medium.com/optuna/scaling-up-optuna-with-ray-tune-88f6ca87b8c7#:~:text=Ray%20Tune%20integrates%20with%20many,is%20one%20of%20these%20frameworks!&text=Even%20better%2C%20if%20you%20use,currently%20not%20available%20in%20Optuna.)
 
 
 
 ## # Using PyTorch Lightning with Tune
 
 文档在这里
 [https://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html](https://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html)
 
 
 ## PyTorch Lightning ModelCheckpoint
 [https://pytorch-lightning.readthedocs.io/en/stable/common/weights\_loading.html](https://pytorch-lightning.readthedocs.io/en/stable/common/weights_loading.html)
 
 ```python
 from pytorch\_lightning.callbacks import ModelCheckpoint

class LitAutoEncoder(LightningModule):
    def validation\_step(self, batch, batch\_idx):
        x, y \= batch
        y\_hat \= self.backbone(x)
        loss \= F.cross\_entropy(y\_hat, y)
        self.log('val\_loss', loss)

\# saves a file like: my/path/sample-mnist-epoch=02-val\_loss=0.32.ckpt
checkpoint\_callback \= ModelCheckpoint(
    monitor\='val\_loss',
    dirpath\='my/path/',
    filename\='sample-mnist-{epoch:02d}\-{val\_loss:.2f}',  # 这个很重要，便于选择最优的检出点
    save\_top\_k\=3,
    mode\='min',
)

trainer \= Trainer(callbacks\=\[checkpoint\_callback\])
 
 
 ```
 
 ## #pytorch-lightning -template
 
 [https://github.com/PyTorchLightning/deep-learning-project-template](https://github.com/PyTorchLightning/deep-learning-project-template)
 
 pytorch-lightning流程
 
 ```python
 
 class LitModel(pl.LightningModule):

    def \_\_init\_\_(...):

    def forward(...):

    def training\_step(...)

    def training\_step\_end(...)

    def training\_epoch\_end(...)

    def validation\_step(...)

    def validation\_step\_end(...)

    def validation\_epoch\_end(...)

    def test\_step(...)

    def test\_step\_end(...)

    def test\_epoch\_end(...)

    def configure\_optimizers(...)

    def any\_extra\_hook(...)
 
 ```
 
 
 
 
 
 ## #pytorch-lightning-bolts
 **Pretrained #SOTA Deep Learning models, callbacks and more for research and production with #PyTorch #Lightning and #PyTorch**
 
 
 [https://pypi.org/project/pytorch-lightning-bolts/](https://pypi.org/project/pytorch-lightning-bolts/)